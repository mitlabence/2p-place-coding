{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auto-reload modules (used to develop functions outside this notebook)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check tuned vector angles range: -pi to pi (in exported hdf5 file, seems like it)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "from scipy.stats import zscore,kstest\n",
    "from scipy.signal import find_peaks\n",
    "import random\n",
    "import os\n",
    "import h5py\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "import seaborn as sns\n",
    "\n",
    "import sys  # for importing one level above\n",
    "sys.path.append(\"..\")\n",
    "from placecode.spatial_coding_functions import *\n",
    "from placecode.utils import open_file, open_dir, read_spatial\n",
    "from placecode.analysis_info import ExpInfo, AnalysisParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perform_ks = True  # perform Kolmogorov-Smirnov test? It takes quite a while.\n",
    "save_results = True  # flag whether to save results into hdf5 file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_expinfo = open_file(\"Select experiment info json file!\")\n",
    "exp_info = ExpInfo(fpath_expinfo)  # TODO: mount data folder! so we can access it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open analysis parameters, append mouse/session-specific data, save it later with results.\n",
    "fpath_analysis_params = open_file(\"Select analysis parameters json file!\")\n",
    "analysis_params = AnalysisParams(fpath_analysis_params)\n",
    "analysis_params.read_exp_info(exp_info)  # extract necessary experiment information for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure the necessary files exist\n",
    "assert os.path.exists(exp_info.fpath_caim)\n",
    "assert os.path.exists(exp_info.fpath_loco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select folder to save results\n",
    "output_folder = open_dir(\"Select folder for output\")  \n",
    "# TODO: add output folder to analysis parameters? Create/check folder mouse_id -> condition, save results there"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CaImAn data\n",
    "with h5py.File(exp_info.fpath_caim, \"r\") as hf_caim:\n",
    "    # temporal\n",
    "    temporal_raw=hf_caim['estimates']['C'][()]\n",
    "    n_components, n_frames = temporal_raw.shape\n",
    "    # access a single temporal component as temporal_raw[i]\n",
    "\n",
    "    # spatial\n",
    "    resolution = hf_caim[\"dims\"][()]\n",
    "    A_data = hf_caim[\"estimates\"][\"A\"][\"data\"][()]\n",
    "    A_indices = hf_caim[\"estimates\"][\"A\"][\"indices\"][()]\n",
    "    A_indptr = hf_caim[\"estimates\"][\"A\"][\"indptr\"][()]\n",
    "    A_shape = hf_caim[\"estimates\"][\"A\"][\"shape\"][()]\n",
    "    spatial = read_spatial(A_data, A_indices, A_indptr, A_shape, n_components, resolution, unflatten=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load loco data\n",
    "# TODO: include stripes, distance per round, etc. in loco data cut to scanner time frame. Missing in Martin's code?\n",
    "#   check https://github.com/mitlabence/matlab-2p/issues/11\n",
    "dict_loco = dict()\n",
    "with h5py.File(exp_info.fpath_loco, \"r\") as hf_loco:\n",
    "    for dset_name in hf_loco[\"inferred\"][\"belt_scn_dict\"].keys():\n",
    "        dtype = np.int16 if dset_name in [\"round\", \"rounds\", \"stripes\"] else np.float64\n",
    "        dict_loco[dset_name] = hf_loco[\"inferred\"][\"belt_scn_dict\"][dset_name][()].astype(dtype)\n",
    "print(dict_loco.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create z-score of temporal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_z = zscore(temporal_raw, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create binary trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_binary = np.zeros(shape=temporal_raw.shape, dtype=bool)\n",
    "for i_unit in range(n_components):\n",
    "    idx_peaks = find_peaks(temporal_raw[i_unit], height=analysis_params.peak_threshold, distance=analysis_params.peak_distance)[0]\n",
    "    temporal_binary[i_unit][idx_peaks] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter rounds\n",
    "Only use rounds where the total length adds up to the expected belt length, and filter by locomotion criteria as defined in Danielson et al. 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_distance=exp_info.belt_length_mm\n",
    "lv_rounds = dict_loco[\"rounds\"].copy()  \n",
    "lv_distPR = dict_loco[\"distance\"].copy() # belt_scn_dict hast distance per round as distance, see issue above\n",
    "lv_speed = dict_loco[\"speed\"].copy()\n",
    "n_rounds=lv_rounds.max()  # number of finished rounds\n",
    "rounds = []\n",
    "round_flags = np.zeros(n_rounds, dtype=np.int8)  # 1 if corresponding round included in analysis, 0 otherwise\n",
    "\n",
    "for round in range(1,n_rounds+1):\n",
    "    dist_current_round=lv_distPR[lv_rounds==round][-1]\n",
    "    #print(dist_current_round)\n",
    "    if abs(dist_current_round-expected_distance)<15:\n",
    "        rounds.append(round-1)\n",
    "        round_flags[round-1] = 1\n",
    "    else:\n",
    "        print(f\"Not using {round}\")\n",
    "\n",
    "\n",
    "num_rounds=len(rounds)\n",
    "print(f'Rounds (starting with 0): {rounds}\\n Number of rounds used: {num_rounds}, total {n_rounds}')\n",
    "\n",
    "# save as parameter a binary array on which round was included\n",
    "analysis_params.rounds_included = round_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the data\n",
    "included_rounds_frames =  np.isin(dict_loco[\"rounds\"], np.array(rounds))  # filter to only those rounds that count (rounds 0-indexing)\n",
    "loco_frames = get_running_frames(dict_loco[\"speed\"], min_peak_speed=0.1)  # TODO: convert labview speed to cm/s somehow? Maybe need original (100 Hz) data\n",
    "idx_filtered = np.logical_and(included_rounds_frames, loco_frames)\n",
    "\n",
    "dict_loco_cut = dict()\n",
    "for k in dict_loco:\n",
    "    dict_loco_cut[k] = dict_loco[k][idx_filtered]\n",
    "temporal_raw_cut = temporal_raw[:, idx_filtered]\n",
    "temporal_z_cut = temporal_z[:, idx_filtered]\n",
    "temporal_binary_cut = temporal_binary[:, idx_filtered]\n",
    "rounds_cut = lv_rounds[idx_filtered]  # get corresponding round index\n",
    "distPR_cut = lv_distPR[idx_filtered]\n",
    "lv_speed_cut = lv_speed[idx_filtered]\n",
    "\n",
    "included_frames = np.arange(0, n_frames)\n",
    "included_frames_cut = included_frames[idx_filtered]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate signal strength map (originally firing rate map)\n",
    "Defined as the average of the temporal component Z-score over one spatial bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = analysis_params.n_bins\n",
    "n_units = n_components\n",
    "analysis_params.n_units = n_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_borders = np.linspace(0, analysis_params.belt_length_mm, analysis_params.n_bins, endpoint=True)\n",
    "#  get signal strength maps\n",
    "ssm_raw = signal_strength_map(temporal_raw_cut, dict_loco_cut[\"rounds\"], dict_loco_cut[\"distance\"], n_bins)\n",
    "ssm_z = signal_strength_map(temporal_z_cut, dict_loco_cut[\"rounds\"], dict_loco_cut[\"distance\"], n_bins)\n",
    "# average over rounds\n",
    "ssm_raw_avg = np.mean(ssm_raw, axis=1)\n",
    "ssm_z_avg = np.mean(ssm_z, axis=1)\n",
    "# create binary mask with 1 where events in ssm appear\n",
    "ssm_events_mask = make_binary(ssm_z,peak_threshold=analysis_params.peak_threshold,peak_distance=analysis_params.peak_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select \"active units\"\n",
    "These are units that show at least `AnalysisParams.n_events_threshold` \"events\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_events_threshold = analysis_params.n_events_threshold\n",
    "idx_active_units = np.where(np.sum(ssm_events_mask, axis=(1, 2)) >= n_events_threshold )[0]  # sum up flags for each spatial bin in neach round\n",
    "percent_active_units = len(idx_active_units)/ssm_events_mask.shape[0]\n",
    "print(f\"Using {percent_active_units*100:.2f}% of all units (at least {n_events_threshold} firing events)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check processed data\n",
    "Plot locomotion (grey) and temporal component (green). Show frames included in analysis (blue horizontal lines) and firing events that are included (vertical lines color coded (10 separate colors) based on bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make scaling factor global (find global max in loco and in temporal components) to easily compare loco and Ca spikes across rounds and across cells\n",
    "bin_beginnings = np.linspace(0, analysis_params.belt_length_mm, analysis_params.n_bins, endpoint=False)  # the mm value of the beginning of each bin\n",
    "cmap = mpl.colormaps[\"tab10\"]\n",
    "def plot_debug(i_cell):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12, 24))\n",
    "    offset = 0.0\n",
    "    axs[0].imshow(ssm_z[i_cell])\n",
    "    axs[0].set_ylim((-1, np.max(rounds)+1))\n",
    "    for round in rounds:\n",
    "        idx_current_round = lv_rounds == round\n",
    "        temporal_current_round = temporal_z[i_cell][idx_current_round]\n",
    "        speed_current_round = lv_speed[idx_current_round]\n",
    "        included_frames_current_round = idx_filtered[idx_current_round]\n",
    "        dist_current_round = lv_distPR[idx_current_round]\n",
    "        # get all spikes that occur during the included frames\n",
    "        events_during_included_frames = np.logical_and(temporal_binary[i_cell][idx_current_round], included_frames_current_round)\n",
    "        firing_current_round = np.nonzero(events_during_included_frames) \n",
    "        # get corresponding spatial bin for each firing event \n",
    "        # find first element greater than distance at which cell fired (index of next bin)\n",
    "        # subtract 1 to find the bin index\n",
    "        i_bins_events = np.searchsorted(bin_beginnings, dist_current_round[firing_current_round], side=\"right\") - 1\n",
    "        # get corresponding indices of color map \n",
    "        i_event_colors = i_bins_events%len(cmap.colors)\n",
    "        # create locomotion segments as (loco_begin_frame, loco_end_frame)\n",
    "        included_segments = []\n",
    "        i_begin_segment = 0\n",
    "        for i in range(1, len(included_frames_current_round)):\n",
    "            # detect beginning of a segment: previous frame not included, current included\n",
    "            if included_frames_current_round[i] and not included_frames_current_round[i-1]:\n",
    "                i_begin_segment = i\n",
    "            # detect end of a segment: previous frame included, current not included\n",
    "            elif not included_frames_current_round[i] and included_frames_current_round[i-1]:\n",
    "                i_end_segment = i-1\n",
    "                # end of segment found: add segment to list\n",
    "                included_segments.append((i_begin_segment, i_end_segment))\n",
    "            # for last frame: if it is part of last segment, it has to be added to that segment.\n",
    "            # If it is a new segment, this new segment of length 1 has to be added\n",
    "            elif i == len(included_frames_current_round) - 1:\n",
    "                if included_frames_current_round[i]:\n",
    "                    if included_frames_current_round[i-1]:\n",
    "                        i_end_segment = i\n",
    "                        included_segments.append((i_begin_segment, i_end_segment))\n",
    "                else:\n",
    "                    included_segments.append((i, i))\n",
    "\n",
    "        temp_min = np.min(temporal_current_round)\n",
    "        temp_max = np.max(temporal_current_round)\n",
    "        speed_min = np.min(speed_current_round)\n",
    "        speed_max = np.max(speed_current_round)\n",
    "        axs[1].hlines(y=[offset-0.05 for i in range(len(included_segments))], xmin=[included_segments[i][0] for i in range(len(included_segments))], xmax=[included_segments[i][1] for i in range(len(included_segments))], linewidth=3 )\n",
    "        # plot the locomotion velocity\n",
    "        if speed_max != speed_min:\n",
    "            axs[1].plot((speed_current_round - speed_min)/(speed_max - speed_min) + offset, color=\"grey\")\n",
    "        else:  # flat line, should be simply offset\n",
    "            axs[1].plot((speed_current_round - speed_min) + offset, color=\"grey\")\n",
    "        offset += 1.1\n",
    "        # plot scaled calcium trace\n",
    "        if temp_max != temp_min:\n",
    "            axs[1].plot((temporal_current_round - temp_min)/(temp_max - temp_min) + offset, color=\"lightgreen\")\n",
    "        else:  # flat line\n",
    "            axs[1].plot((temporal_current_round - temp_min) + offset, color=\"lightgreen\")\n",
    "        # plot all firing events that were recorded\n",
    "        axs[1].vlines(x=firing_current_round, ymin=offset-0.05, ymax=offset+1.05, linewidth=1, color=[cmap.colors[i_color] for i_color in i_event_colors])\n",
    "        offset += 1.1\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot ssm along with traces? (convert time frame index into spatial bin index, i.e. plot (i_frame, ssm_z[spatials[i_frame]]))\n",
    "interact(plot_debug, i_cell=widgets.IntSlider(min=0, max=n_components-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate original and shuffled mean place coding vector lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles_on_belt = np.linspace(0, 2*np.pi, n_bins, endpoint=False) #initializing the circle with 150 bins corresponding to the distance on the belt\n",
    "assert len(angles_on_belt) == n_bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning vector analysis\n",
    "Loosely based on Danielson et al. 2016: Distinct Contribution of Adult-Born Hippocampal Granule Cells to Context Encoding.\n",
    "To calculate a tuning vector when spatial bins for one recording are given:\n",
    "1. Get the location of each bin marked as having an event (`ssm_events_mask[i_cell] == 1`).\n",
    "2. Get vector sum where each vector has unit length. \n",
    "3. Divide by number of vectors.\n",
    "\n",
    "The algorithm:\n",
    "1. Calculate original tuning vector for single cell\n",
    "2. Shuffle bins within each round, find event flags, get tuning vector with new bins and flags.\n",
    "3. Check the ratio of (# shuffled vectors shorter than original vector)/(# shuffles). This is the p value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm_events_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cells not in idx_active_units will have np.nan, everything else overwritten\n",
    "p_values_tuned = np.full(n_units, np.nan)  # p=1 is insignificance, suitable for cells which are not even included in analysis (not enough events). np.zeros would mark them as extremely place coding\n",
    "shuffled_tuned_vector_lengths = np.full((n_units, analysis_params.n_shuffle), np.nan)\n",
    "tv_lengths = np.full(n_units, np.nan)\n",
    "tv_angles = np.full(n_units, np.nan)  # radians, 0 to 2pi range\n",
    "\n",
    "for i_cell in idx_active_units:  # only use units with enough events\n",
    "    ssm_cell = ssm_z[i_cell]\n",
    "    ssm_events_cell = ssm_events_mask[i_cell]\n",
    "    # add up the bins with corresponding angles. Ignore 0-length vectors for easier normalization with number of vectors\n",
    "    i_rounds, i_bins = np.where(ssm_events_cell > 0)  # first array in tuple gives back row, second the column indices\n",
    "    event_angles = angles_on_belt[i_bins]\n",
    "    event_radii = ssm_cell[i_rounds, i_bins]\n",
    "    tv_length, tv_angle = vector_sum(event_radii, event_angles )\n",
    "    n_vectors = len(event_radii)  \n",
    "    tv_length = tv_length/n_vectors  # normalize by number of vectors that were added up\n",
    "    tv_lengths[i_cell] = tv_length\n",
    "    tv_angles[i_cell] = tv_angle\n",
    "    # Tuned vector length and angle for shuffled data\n",
    "    tv_angles_shuffled = np.zeros(analysis_params.n_shuffle, dtype=np.float64)\n",
    "    tv_lengths_shuffled = np.zeros(analysis_params.n_shuffle, dtype=np.float64)\n",
    "    for i_shuffle in range(analysis_params.n_shuffle):\n",
    "        #shuffle each row separately\n",
    "        ssm_shuffled = ssm_cell.copy()  # Make a copy to avoid modifying original data\n",
    "        for spiking_current_round in ssm_shuffled:\n",
    "            np.random.shuffle(spiking_current_round)  # Shuffle the spiking events within each round independently\n",
    "        # find flags again. \n",
    "        ssm_events_cell_shuffled = make_binary(ssm_shuffled, peak_threshold=analysis_params.peak_threshold,peak_distance=analysis_params.peak_distance)\n",
    "        # Calculate mean direction and magnitude\n",
    "        i_rounds_shuffled, i_bins_shuffled = np.where(ssm_events_cell_shuffled > 0)  # first array in tuple gives back row, second the column indices\n",
    "        event_angles_shuffled = angles_on_belt[i_bins_shuffled]\n",
    "        event_radii_shuffled = ssm_shuffled[i_rounds_shuffled, i_bins_shuffled]\n",
    "        shuffled_length, shuffled_angle = vector_sum(event_radii_shuffled, event_angles_shuffled)    \n",
    "        tv_lengths_shuffled[i_shuffle] = shuffled_length/n_vectors  # normalize by number of vectors that was added up\n",
    "        tv_angles_shuffled[i_shuffle] = shuffled_angle\n",
    "    shuffled_tuned_vector_lengths[i_cell] = tv_lengths_shuffled\n",
    "    # calculate p-value as the percentile in which the candidate cell lies\n",
    "    # large p_value (>0.95) signifies place coding\n",
    "    p_value = float(np.sum(tv_lengths_shuffled >= tv_length)) / float(analysis_params.n_shuffle)\n",
    "    p_values_tuned[i_cell] = p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolmogorov-Smirnov test\n",
    "Compare the measured data to a random shuffle (baseline). Then compare n_shuffle shuffles to the same baseline.\n",
    "1. Randomly shift the spatial bins (Z-score) of each round to get a shuffled ssm. Then apply KS test to the original and shuffled (\"baseline\") bins, both averaged over rounds. This is the baseline statistic.\n",
    "2. Shuffle the original data 1000 times, each time randomly shifting the spatial bins of each round. Compare the averaged bins of this shuffle and the baseline bins with the KS test.\n",
    "3. Calculate the p-value as the ratio of (# [KS statistic of shuffled] > [KS statistic of baseline])/(# shuffles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values_ks = np.full(n_units, np.nan)  # p=1 is insignificance, suitable for cells which are not even included in analysis (not enough events). np.zeros would mark them as extremely place coding\n",
    "shuffled_data_ks = np.full((n_units, analysis_params.n_shuffle), np.nan)\n",
    "shuffled_bl_ks = np.full(n_units, np.nan)\n",
    "if perform_ks:\n",
    "    for i_cell in idx_active_units:\n",
    "        ssm_cell = ssm_z[i_cell]\n",
    "        # acquire baseline\n",
    "        cell_spiking_bl = np.zeros(ssm_cell.shape)\n",
    "        for i_round in range(len(cell_spiking_bl)):\n",
    "                shift = random.randint(0, analysis_params.n_bins)\n",
    "                cell_spiking_bl[i_round] = np.roll(ssm_cell[i_round], shift)  # Shuffle the spiking events within each round independently\n",
    "        experiment_avg = np.mean(ssm_cell, axis=0)\n",
    "        baseline_avg=np.mean(cell_spiking_bl,axis=0)\n",
    "        ks_baseline,_=kstest(experiment_avg,baseline_avg)\n",
    "        shuffled_bl_ks[i_cell] = ks_baseline\n",
    "        # shuffle\n",
    "        ks_shuffled = np.zeros(analysis_params.n_shuffle)\n",
    "        for i_shuffle in range(analysis_params.n_shuffle):\n",
    "            ssm_shuffled = np.zeros(ssm_cell.shape)\n",
    "            for i_round in range(len(ssm_shuffled)):\n",
    "                shift = random.randint(0, analysis_params.n_bins)  # TODO: shift lower range should be 1 or 0?\n",
    "                ssm_shuffled[i_round] = np.roll(ssm_cell[i_round], shift)  # Shuffle the spiking events within each round independently\n",
    "            shuffled_avg = np.mean(ssm_shuffled, axis=0)\n",
    "            ks_shuffle,p_value_=kstest(baseline_avg, shuffled_avg)\n",
    "            ks_shuffled[i_shuffle] = ks_shuffle\n",
    "        shuffled_data_ks[i_cell] = ks_shuffled\n",
    "        # calculate p value\n",
    "        p_value_ks = np.sum(ks_shuffled >= ks_baseline) / len(ks_shuffled)\n",
    "        p_values_ks[i_cell] = p_value_ks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make an ultimate debug plot:\n",
    "# plot the trace per each round, and underline the frames that are included for analysis (when I do cuts, also cut for the frame indices in the end).\n",
    "# Plot locomotion as well, to see that only frames with locomotion are included. Also plot the binary spikes. Then plot the spatial bins too? \n",
    "# As vlines or different colored spikes for each spatial bin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal strength per bin\n",
    "Columns:  cell no., bin index, avg Signal strength over rounds, p_tuning, p_ks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signal strength per cell per round per bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm_z_active = ssm_z[idx_active_units]\n",
    "ssm_z_active_avg = np.mean(ssm_z_active, axis=1)  # average across rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssm_z_active.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_entries = ssm_z_active.flatten().shape[0]\n",
    "col_cell_ids = np.zeros(ssm_z_active.shape, dtype=np.int16)\n",
    "col_rounds = np.zeros(ssm_z_active.shape, dtype=np.int16)\n",
    "col_bin_idxs = np.zeros(ssm_z_active.shape, dtype=np.int16)\n",
    "# TODO loop over cells, rounds, and bins, add cell_id, round, bin_idx, firing_rate\n",
    "for i_cell in range(ssm_z_active.shape[0]):\n",
    "    col_cell_ids[i_cell] = np.full((ssm_z_active.shape[1], ssm_z_active.shape[2]), idx_active_units[i_cell], dtype=np.int16)\n",
    "    for i_round in range(ssm_z.shape[1]):\n",
    "        col_rounds[i_cell][i_round] = np.full(ssm_z.shape[2], i_round, dtype=np.int16)\n",
    "        col_bin_idxs[i_cell][i_round] = np.linspace(0, ssm_z_avg.shape[1], num=ssm_z_avg.shape[1], endpoint=False, dtype=np.int16)\n",
    "dict_fr={\"cell_id\":col_cell_ids.flatten(), \"round\":col_rounds.flatten(), \"bin_idx\":col_bin_idxs.flatten(), \"firing_rate\":ssm_z_active.flatten()}\n",
    "df_firing_rate = pd.DataFrame(dict_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Averaged signal strength over rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_entries = ssm_z_avg.flatten().shape[0]\n",
    "col_cell_ids_avg = np.zeros(ssm_z_active_avg.shape, dtype=np.int16)\n",
    "col_bin_idxs_avg = np.zeros(ssm_z_active_avg.shape, dtype=np.int16)\n",
    "# TODO loop over cells, rounds, and bins, add cell_id, round, bin_idx, firing_rate\n",
    "for i_cell in range(ssm_z_active_avg.shape[0]):\n",
    "    col_cell_ids_avg[i_cell] = np.full(ssm_z_active_avg.shape[1], idx_active_units[i_cell])\n",
    "    col_bin_idxs_avg[i_cell] = np.linspace(0, ssm_z_active_avg.shape[1], num=ssm_z_active_avg.shape[1], endpoint=False, dtype=np.int16)\n",
    "dict_fr={\"cell_id\":col_cell_ids_avg.flatten(), \"bin_idx\":col_bin_idxs_avg.flatten(), \"firing_rate\":ssm_z_active_avg.flatten()}\n",
    "df_avg_firing_rate = pd.DataFrame(dict_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P values per cell for both methods and bin index for first maximum of signal strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_p_vals = {\"cell_id\": np.array([i for i in range(n_units)]), \"i_fr_max\": np.argmax(ssm_z_avg, axis=1), \"p_tuning\": p_values_tuned, \"p_ks\": p_values_ks}\n",
    "df_p_values = pd.DataFrame(data=dict_p_vals)\n",
    "del dict_p_vals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_firing_rate = df_avg_firing_rate.join(df_p_values, how=\"left\", on=\"cell_id\", rsuffix=\"_other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot signal strength over belt for place cells vs non-place cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuned vector method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_firing_rate_accepted = df_avg_firing_rate[df_avg_firing_rate[\"p_tuning\"] <= 0.05].join(df_p_values, on=\"cell_id\", how=\"left\", rsuffix=\"_other\").sort_values(by=[\"i_fr_max\", \"bin_idx\"]).pivot_table(index=\"cell_id\", columns=\"bin_idx\", values=\"firing_rate\", sort=False)\n",
    "pivot_firing_rate_rejected = df_avg_firing_rate[df_avg_firing_rate[\"p_tuning\"] > 0.05].join(df_p_values, on=\"cell_id\", how=\"left\", rsuffix=\"_other\").sort_values(by=[\"i_fr_max\", \"bin_idx\"]).pivot_table(index=\"cell_id\", columns=\"bin_idx\", values=\"firing_rate\", sort=False)\n",
    "\n",
    "f, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
    "sns.heatmap(pivot_firing_rate_accepted, ax=axs[0])\n",
    "sns.heatmap(pivot_firing_rate_rejected, ax=axs[1])\n",
    "\n",
    "axs[0].set_title(\"accepted\")\n",
    "axs[1].set_title(\"rejected\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KS method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_ks:\n",
    "    pivot_firing_rate_accepted = df_avg_firing_rate[df_avg_firing_rate[\"p_ks\"] <= 0.05].join(df_p_values, on=\"cell_id\", how=\"left\", rsuffix=\"_other\").sort_values(by=[\"i_fr_max\", \"bin_idx\"]).pivot_table(index=\"cell_id\", columns=\"bin_idx\", values=\"firing_rate\", sort=False)\n",
    "    pivot_firing_rate_rejected = df_avg_firing_rate[df_avg_firing_rate[\"p_ks\"] > 0.05].join(df_p_values, on=\"cell_id\", how=\"left\", rsuffix=\"_other\").sort_values(by=[\"i_fr_max\", \"bin_idx\"]).pivot_table(index=\"cell_id\", columns=\"bin_idx\", values=\"firing_rate\", sort=False)\n",
    "\n",
    "    f, axs = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    sns.heatmap(pivot_firing_rate_accepted, ax=axs[0])\n",
    "    sns.heatmap(pivot_firing_rate_rejected, ax=axs[1])\n",
    "    axs[0].set_title(\"accepted\")\n",
    "    axs[1].set_title(\"rejected\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell-specific figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_cell = False\n",
    "i_cell = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity per round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_single_cell:\n",
    "    #pivot_firing_rate_accepted = df_avg_firing_rate[df_avg_firing_rate[\"p_ks\"] > 0.05].join(df_p_values, on=\"cell_id\", how=\"left\", rsuffix=\"_other\").sort_values(by=[\"i_fr_max\", \"bin_idx\"]).pivot_table(index=\"cell_id\", columns=\"bin_idx\", values=\"firing_rate\", sort=False)\n",
    "    pivot_table_single = df_firing_rate[df_firing_rate[\"cell_id\"] == i_cell].pivot_table(columns=\"bin_idx\", index=\"round\", values=\"firing_rate\")\n",
    "    f, ax = plt.subplots(figsize=(9, 6))\n",
    "    sns.heatmap(pivot_table_single, ax=ax)\n",
    "    ax.set_title(f\"Cell #{i_cell}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_single_cell:\n",
    "    fig, axs = plt.subplots(1,2, figsize=(12, 4))\n",
    "    axs[0].hist(shuffled_tuned_vector_lengths[i_cell])\n",
    "    axs[1].hist(shuffled_data_ks[i_cell])\n",
    "\n",
    "    axs[0].vlines(x=tv_lengths[i_cell], ymin=0, ymax=100, color=\"red\")\n",
    "    axs[1].vlines(x=shuffled_bl_ks[i_cell], ymin=0, ymax=100, color=\"red\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circular plotting of activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_single_cell:\n",
    "    i_cell = 359\n",
    "    binary_spiking_cell = ssm_events_mask[i_cell]\n",
    "    n_bins = len(binary_spiking_cell[0])\n",
    "    fig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\n",
    "    ax.set_xlabel('Angle (degrees)')\n",
    "    ax.set_ylabel('Radius')\n",
    "    for i_round in range(len(binary_spiking_cell)):\n",
    "        i_firing_bins = binary_spiking_cell[i_round].nonzero()[0]\n",
    "        if len(i_firing_bins) > 0:\n",
    "            tv_length = i_round + 1  # avoid 0 length\n",
    "            vector_lengths = [tv_length]*len(i_firing_bins)\n",
    "            firing_angles_deg = i_firing_bins*2*math.pi/n_bins\n",
    "            ax.scatter(firing_angles_deg, vector_lengths)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(i_active_cell):\n",
    "    i_cell = idx_active_units[i_active_cell]\n",
    "    pivot_table_single = df_firing_rate[df_firing_rate[\"cell_id\"] == i_cell].pivot_table(columns=\"bin_idx\", index=\"round\", values=\"firing_rate\")\n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    binary_spiking_cell = ssm_events_mask[i_cell]\n",
    "    n_bins = len(binary_spiking_cell[0])\n",
    "    ax1 = plt.subplot(221)\n",
    "    ax2 = plt.subplot(222, projection='polar')\n",
    "    ax3 = plt.subplot(223)\n",
    "    ax4  =plt.subplot(224)\n",
    "    sns.heatmap(pivot_table_single, ax=ax1)\n",
    "    ax2.set_xlabel('Angle (degrees)')\n",
    "    ax2.set_ylabel('Round')\n",
    "    vector_lengths, i_bins = np.where(binary_spiking_cell)  # vector lengths will be round index, starting with 1\n",
    "    vector_lengths += 1\n",
    "    firing_angles_rad = i_bins*2*np.pi/n_bins\n",
    "    ax2.scatter(firing_angles_rad, vector_lengths)\n",
    "    ax2.set_ylim((0, len(binary_spiking_cell)+0.2))\n",
    "    ax1.set_title(f\"Cell #{i_cell}\")\n",
    "    ax3.set_title(f\"Tuning vector length vs shuffled data, p={p_values_tuned[i_cell]}\")\n",
    "    ax3.hist(shuffled_tuned_vector_lengths[i_cell], bins=20)\n",
    "    ax3.vlines(x=tv_lengths[i_cell], ymin=0, ymax=100, color=\"red\")\n",
    "    if perform_ks:\n",
    "        ax4.hist(shuffled_data_ks[i_cell], bins=20)\n",
    "        ax4.vlines(x=shuffled_bl_ks[i_cell], ymin=0, ymax=100, color=\"red\")\n",
    "        ax4.set_title(f\"K-S test, p={p_values_ks[i_cell]}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(p_values_tuned, p_values_ks)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(plot_data, i_active_cell=widgets.IntSlider(min=0, max=len(idx_active_units)-1, step=1, value=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath_output = os.path.join(output_folder, os.path.splitext(os.path.split(fpath_expinfo)[-1])[0].replace(\"expinfo\", \"placecoding\")+\".h5\")\n",
    "print(f\"Saving to {fpath_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_results:\n",
    "    with h5py.File(fpath_output, \"w\") as hf:\n",
    "        dict_analysis_params = analysis_params.to_dict()\n",
    "        for key in dict_analysis_params.keys():\n",
    "            if dict_analysis_params[key] is not None:\n",
    "                hf.attrs[key] = dict_analysis_params[key]\n",
    "            else:\n",
    "                hf.attrs[key] = np.nan\n",
    "        dict_exp_info = exp_info.to_dict()\n",
    "        for key in dict_exp_info.keys():\n",
    "            if dict_exp_info[key] is not None:\n",
    "                hf.attrs[key] = dict_exp_info[key]\n",
    "            else:\n",
    "                hf.attrs[key] = np.nan\n",
    "        # temporal and spatial components\n",
    "        hf.create_dataset(\"temporal_raw\", data=temporal_raw)\n",
    "        hf.create_dataset(\"A_data\", data=A_data)\n",
    "        hf.create_dataset(\"A_indices\", data=A_indices)\n",
    "        hf.create_dataset(\"A_indptr\", data=A_indptr)\n",
    "        hf.create_dataset(\"A_shape\", data=A_shape)\n",
    "        hf.attrs[\"resolution\"] = resolution\n",
    "\n",
    "        hf.create_dataset(\"ssm_raw\", data=ssm_raw)\n",
    "        hf.create_dataset(\"ssm_z\", data=ssm_z)\n",
    "        hf.create_dataset(\"ssm_events_mask\", data=ssm_events_mask)\n",
    "        hf.create_dataset(\"tuned_vector_lengths\", data=tv_lengths)\n",
    "        hf.create_dataset(\"tuned_vector_angles\", data=tv_angles)\n",
    "        hf.create_dataset(\"shuffled_tuned_vector_lengths\", data=shuffled_tuned_vector_lengths)\n",
    "        hf.create_dataset(\"p_values_tuned\", data=p_values_tuned)\n",
    "        hf.create_dataset(\"shuffled_data_ks\", data=shuffled_data_ks)\n",
    "        hf.create_dataset(\"shuffled_bl_ks\", data=shuffled_bl_ks)\n",
    "        hf.create_dataset(\"p_values_ks\", data=p_values_ks)\n",
    "        hf.create_dataset(\"idx_active_units\", data=idx_active_units)\n",
    "\n",
    "        # TODO: add spatial components (to make it a standalone file) as sparse matrix\n",
    "    print(f\"Saved to {fpath_output}\")\n",
    "else:\n",
    "    print(\"Not saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "placecoding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
