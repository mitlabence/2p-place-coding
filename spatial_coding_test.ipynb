{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import h5py\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import scipy.sparse \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore,kstest\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.signal import find_peaks\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import random\n",
    "import os\n",
    "\n",
    "#import my functions\n",
    "from spatial_coding_functions import event_numbers,make_firing_rate_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all good with naming the files. continue\n"
     ]
    }
   ],
   "source": [
    "#initializing the foler with the data\n",
    "\n",
    "mouse_ID='WEZ8917'\n",
    "condition='24hr_poststim'\n",
    "\n",
    "home_folder='C:\\\\Users\\\\Theodore Tamiolakis\\\\OneDrive\\\\PhD\\\\Lab'\n",
    "\n",
    "data_folder=f'{home_folder}\\\\data\\\\representational_drift' #r means that I will treat the string as a raw string (/ are special characters)\n",
    "results_folder=f'{home_folder}\\\\Results\\\\representational_drift\\\\{mouse_ID}\\\\{condition}\\\\'\n",
    "\n",
    "# Check if results folder exists, create it if not\n",
    "if not os.path.exists(results_folder):\n",
    "    os.makedirs(results_folder)\n",
    "\n",
    "cnmf_file=glob.glob(f\"{data_folder}\\\\231108_{mouse_ID}_{condition}*.hdf5\") #data from cnmf\n",
    "belt_file=glob.glob(f\"{data_folder}\\\\231108_{mouse_ID}_{condition}*.h5\") #data from lab view and lfp\n",
    "\n",
    "#check that there is only one file out of each\n",
    "\n",
    "if len(cnmf_file)>1 or len(belt_file)>1:\n",
    "    print (\"error. two files with similar conditions found\")\n",
    "else:\n",
    "    print (\"all good with naming the files. continue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since glob glob is a list I take the first element out of belt file and cmf file. I have checked begore that it is only one\n",
    "cnmf_file,belt_file=cnmf_file[0],belt_file[0]\n",
    "\n",
    "#preprocessing the data\n",
    "\n",
    "#opening the hpf5 file\n",
    "fluo_hdf=h5py.File(cnmf_file)['estimates']['C']\n",
    "#convert it in a dataframe\n",
    "fluo_hdf=pd.DataFrame(fluo_hdf)\n",
    "\n",
    "\n",
    "#opening the stripe folder\n",
    "stripe_hdf=h5py.File(belt_file)['inferred']['belt_dict']['stripes']\n",
    "stripe_hdf=pd.DataFrame(stripe_hdf)\n",
    "fluo_hdf_r=fluo_hdf.T #swap columns and rows in fluo hdf\n",
    "n_cells=fluo_hdf_r.shape[1]\n",
    "\n",
    "#normazing the whole panda frame by applying z score\n",
    "fluo_hdf_r=fluo_hdf_r.apply(zscore,axis=0)\n",
    "#identifying and storing the number of units\n",
    "units_n=fluo_hdf_r.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.creating panda frames for running distance, speed, time and immobility\n",
    "2.adding all of them into the intial panda frame so that I can analyze everything at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# panda frame for time\n",
    "time_hdf=h5py.File(belt_file)['inferred']['belt_dict']['tsscn']\n",
    "time_hdf=pd.DataFrame(time_hdf)\n",
    "time_hdf.columns=['Time (ms)']\n",
    "#panda frame for distance\n",
    "distance_hdf=h5py.File(belt_file)['inferred']['belt_scn_df']['distance']\n",
    "distance_hdf=pd.DataFrame(distance_hdf)\n",
    "distance_hdf.columns=['Distance']\n",
    "# panda frame for speed\n",
    "speed_hdf=h5py.File(belt_file)['inferred']['belt_scn_df']['speed']\n",
    "speed_hdf=pd.DataFrame(speed_hdf)\n",
    "speed_hdf.columns=['Speed']\n",
    "# panda frame for number of rounds\n",
    "rounds_hdf=h5py.File(belt_file)['inferred']['belt_scn_df']['rounds']\n",
    "rounds_hdf=pd.DataFrame(rounds_hdf)\n",
    "rounds_hdf.columns=['Rounds']\n",
    "rounds_hdf=rounds_hdf.astype(int)\n",
    "# panda frame for running(yes or no running)\n",
    "running_hdf=h5py.File(belt_file)['inferred']['belt_scn_df']['running']\n",
    "running_hdf=pd.DataFrame(running_hdf)\n",
    "running_hdf.columns=['Running']\n",
    "running_hdf=running_hdf.astype(int)\n",
    "\n",
    "#####################################################################################################################\n",
    "\n",
    "#adding all the parameters in one panda frame\n",
    "fluo_hdf_r = pd.concat([fluo_hdf_r, time_hdf, distance_hdf, speed_hdf, rounds_hdf, running_hdf], axis=1, ignore_index=True)\n",
    "# Create a mapping dictionary for column renaming\n",
    "rename_mapping = {old_col: new_col for old_col, new_col in zip(fluo_hdf_r.columns[-5:], ['Time (ms)', 'Distance', 'Speed', 'Rounds', 'Running'])}\n",
    "# Rename the columns\n",
    "fluo_hdf_r = fluo_hdf_r.rename(columns=rename_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to plot as I test the different distances every time in every round to compare.\n",
    "all the rounds should be approximately 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 0,1500.0\n",
      "round 1,1498.0015987210231\n",
      "round 2,1495.0\n",
      "round 3,1500.0\n",
      "round 4,1497.1751412429378\n",
      "round 5,1497.4511469838571\n",
      "round 6,1500.0\n",
      "round 7,1492.6289926289926\n"
     ]
    }
   ],
   "source": [
    "rounds=[]\n",
    "for i in range(8):\n",
    "    dis=fluo_hdf_r[fluo_hdf_r['Rounds']==i].iloc[-1]['Distance']\n",
    "    rounds.append(dis)\n",
    "    print (f\"round {i},{dis}\")\n",
    "\n",
    "\n",
    "\n",
    "#all the distances approximately 1.5 m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "processing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=fluo_hdf_r                     #renaming data so that it is easier to process them\n",
    "data=data[data['Speed']>0]   #taking only the timepoints when the mouse moves\n",
    "data=data[data['Rounds']<8]         #only taking the data of 8 rounds because this is where the belt approximates 150 cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate spatial firing map for each round indivudually. then I can average them easily.\n",
    "no need to do it from the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "fluorescence_traces = data.iloc[:, :units_n]  # Columns containing fluorescence traces\n",
    "distance = data['Distance']  # Column containing distance on the linear belt\n",
    "speed = data['Speed']   # Column containing speed\n",
    "rounds = data['Rounds']   # Column containing the number of rounds\n",
    "running = data['Running'] # Column containing running status\n",
    "\n",
    "\n",
    "num_rounds = 7\n",
    "num_bins = 150\n",
    "num_units=units_n\n",
    "\n",
    "#making the firing rate map for every cell and every round\n",
    "\n",
    "firing_rate_maps=make_firing_rate_maps(data,num_rounds,num_units,num_bins)\n",
    "\n",
    "#calculate the average firing rate map for every cell\n",
    "\n",
    "avr_firing_rate_maps=np.mean(firing_rate_maps,axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(593, 150)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avr_firing_rate_maps.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify the units which have at least a certain amount of events\n",
    "\n",
    "threshold=4 #it means that I take events as something above 4 standrad deviations\n",
    "peak_window=20 #it means that I look for local maxima within 20 cells in the panda frame\n",
    "n_events_threshold=3 #how many events do I want at least per cell\n",
    "\n",
    "cells_with_events=[]\n",
    "for i in range(units_n):\n",
    "    if len(event_numbers(data[i],threshold,peak_window))>n_events_threshold:\n",
    "        cells_with_events.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_place_cells=firing_rate_maps.apply(np.argmax,axis=1)\n",
    "test_filtered = round1.loc[round1_max >= 0]\n",
    "filtered_values=test_filtered.apply(np.argmax, axis=1).sort_values().index\n",
    "test_filtered_sorted = test_filtered.loc[list(filtered_values),:]\n",
    "# Calculate the maximum diagonal values\n",
    "sns.heatmap(test_filtered_sorted,vmax=20,xticklabels=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting\n",
    "1. z score activity per round per cell\n",
    "2. average z-score activity for all the rounds\n",
    "3. spatial vector analysis schematic and histogram with comparison of shuffling data\n",
    "4. statistical significanc e analysis using colmogorov-smirnov test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INITIAL PARAMETERS\n",
    "\n",
    "n_cells=units_n #number of cells that I want to permutate\n",
    "num_bins=150\n",
    "num_rounds=7\n",
    "peak_threshold=3 #amount of sd from the baseline activity\n",
    "peak_distance=10 #distance in cells that I am looking for local maxima\n",
    "belt_length=150 #(in cm)\n",
    "shuffling_times=1000  #time of shuffling for statistical comparison\n",
    "\n",
    "\n",
    "#creating pdfs for significant and non significant cells sop that I can review them after\n",
    "# Create two pdf files\n",
    "significant_cells = PdfPages(results_folder+'significant_place_cells.pdf')\n",
    "non_significant_cells = PdfPages(results_folder+'non_significant_place_cells.pdf')\n",
    "\n",
    "\n",
    "angles = np.linspace(0, 2*np.pi, num_bins, endpoint=False) #initializing the circle with 150 bins corresponding to the distance on the belt\n",
    "\n",
    "place_cells=[]\n",
    "\n",
    "for cell in range(n_cells):\n",
    "\n",
    "    ################################################################################\n",
    "    # intiialing the firing map for every cell and making the binary panda frame with the events for comparison with spatila tuning vector\n",
    "    ###############################################################################\n",
    "\n",
    "    # Update events_data with binarized firing rate map\n",
    "    cell_firing_rate_map=firing_rate_maps[cell]\n",
    "    cell_rate_binarized=np.zeros_like(cell_firing_rate_map)\n",
    "    peak_n=0 #initializing the number of peaks (or events) as zero. I will add later to access the total number of events\n",
    "    for i in range(cell_firing_rate_map.shape[0]):        \n",
    "        data_per_round=cell_firing_rate_map[i]\n",
    "        peaks, _=find_peaks(data_per_round, height=peak_threshold,distance=peak_distance)\n",
    "        peak_n+=len(peaks)\n",
    "        cell_rate_binarized[i][peaks]=1\n",
    "\n",
    "\n",
    "    events_data = cell_rate_binarized  # Assuming you have your actual events data\n",
    "\n",
    "    #events=events_data\n",
    "    if peak_n>4: #I follow the logic from the paper of Danielson and Niko. I only used cells for visualization where I have at least 4 events\n",
    "\n",
    "        ####################################################\n",
    "        # creating the plot where I will put all of the suplots inside\n",
    "        ###################################################\n",
    "\n",
    "        fig, ax = plt.subplots(3, 2, figsize=(15, 15))\n",
    "        fig.suptitle(f'Cell {cell} \\n shuffles {shuffling_times}')\n",
    "\n",
    "        ###################################################\n",
    "        #plotting fluorescence for every round in the first subplot\n",
    "        ##################################################\n",
    "\n",
    "        sns.heatmap(firing_rate_maps[cell],ax=ax[0,0],cbar_kws={'label': 'z-score fluorescence'})\n",
    "        ax[0,0].set_ylabel('Rounds')\n",
    "        ax[0,0].set_title(\"Activity per round\")\n",
    "        ax[0,0].set_xlabel('Position belt (cm)')\n",
    "        ax1_labels=range(1,num_rounds+1)\n",
    "        ax[0,0].set_yticklabels(ax1_labels)\n",
    "        custom_ticks = [0, 50, 100, 150]  # Specify the positions where you want the ticks\n",
    "        custom_labels = ['0', '50', '100', '150']  # Specify the labels for the ticks\n",
    "        ax[0,0].set_xticks(custom_ticks)\n",
    "        ax[0,0].set_xticklabels(custom_labels)\n",
    "\n",
    "        ###################################################\n",
    "        #plotting average z=score for every round in the second subplot\n",
    "        ##################################################\n",
    "\n",
    "\n",
    "        avg_activity = pd.DataFrame(avr_firing_rate_maps[cell], columns=['']) #taking the average activity of the cell and converting it to data frame to plot it\n",
    "        sns.heatmap(avg_activity.transpose(),ax=ax[0,1],cbar_kws={'label': 'z-score fluorescence'})\n",
    "\n",
    "        #changing the size of the second subplot\n",
    "        position = ax[0,1].get_position()  # Get the current position of the subplot\n",
    "        ax[0,1].set_title(\"Average activity\")\n",
    "        position.y0 -= -0.1  # Decrease the bottom boundary\n",
    "        position.y1 *= 0.9  # Decrease the height\n",
    "        ax[0,1].set_position(position)  # Set the new position\n",
    "        ax[0,1].set_ylabel('')\n",
    "        ax[0,1].set_xlabel('Position belt (cm)')\n",
    "        ax[0,1].set_xticks(custom_ticks)\n",
    "        ax[0,1].set_xticklabels(custom_labels)\n",
    "\n",
    "        ###################################################\n",
    "        #plotting spatial tuned vector figure\n",
    "        ##################################################\n",
    "\n",
    "\n",
    "        # Create a polar plot with circles representing each round\n",
    "        \n",
    "        ax[1,0] = plt.subplot(2,2,3,projection='polar')\n",
    "        ax[1,0].set_theta_direction(-1)  # Set clockwise direction\n",
    "        ax[1,0].set_theta_zero_location('N')  # Set zero angle at North\n",
    "\n",
    "        # Customize the tick labels on the circles (bins) so  that I can see the distance on the belt instead of degrees\n",
    "        circle_ticks = [0, 120, 240]  # Tick positions in degrees\n",
    "        circle_labels = ['0/150 cm', '50 cm', '100 cm']  # Labels for the ticks\n",
    "        ax[1,0].set_xticks(np.radians(circle_ticks))  # Set the tick positions in radians\n",
    "        ax[1,0].set_xticklabels(circle_labels)  # Set the tick labels\n",
    "\n",
    "        # Plot events on the circle\n",
    "        for i, events_round in enumerate(events_data):\n",
    "            event_angles = angles[np.where(events_round == 1)] #I found the angles where I have events\n",
    "            event_radii = np.ones_like(event_angles) * (i + 1)  # Adjust the radius for each round\n",
    "            ax[1,0].scatter(event_angles, event_radii, s=100, label=f'Round {i+1}', alpha=0.7)\n",
    "            for angle, radius in zip(event_angles, event_radii):\n",
    "                ax[1,0].plot([0, angle], [0, radius], color='black', linestyle='-', linewidth=1)  # Connect center to event dot. better for visualization\n",
    "\n",
    "        # Calculate average direction using Cartesian coordinates\n",
    "        x_coords = []\n",
    "        y_coords = []\n",
    "        for i, events_round in enumerate(events_data):\n",
    "            event_angles = angles[np.where(events_round == 1)]\n",
    "            event_radii = np.ones_like(event_angles) * (i + 1)  # Adjust the radius for each round\n",
    "            x_coords.extend(np.cos(event_angles)/(i + 1))\n",
    "            y_coords.extend(np.sin(event_angles)/(i + 1))\n",
    "\n",
    "        # Sum the coordinates . I divide it with number of events. instead of number of rounds MAYBE THIS WOULD NEED TO CHANGE\n",
    "        sum_x = np.sum(x_coords)/ peak_n\n",
    "        sum_y = np.sum(y_coords)/ peak_n\n",
    "\n",
    "        # Convert the sum of coordinates to polar coordinates\n",
    "        sum_direction = np.arctan2(sum_y, sum_x)  # Note: y comes before x in arctan2\n",
    "        sum_magnitude = np.sqrt(sum_x**2 + sum_y**2)\n",
    "\n",
    "        # Plot the average tuning vector as an arrow\n",
    "        ax[1,0].arrow(0, 0, sum_direction, sum_magnitude, head_width=0, head_length=0,linewidth=4, fc='red', ec='red')\n",
    "\n",
    "        # Customize plot\n",
    "        ax[1,0].set_title(f'Events and Tuning Vectors Cell {cell}')\n",
    "        ax[1,0].legend(loc='upper left',bbox_to_anchor=(0.9, 1), frameon=False)\n",
    "        ax[1,0].grid(True)\n",
    "\n",
    "        # Convert average direction from radians to degrees\n",
    "        sum_direction_degrees = np.degrees(sum_direction)\n",
    "        # Map the sum direction to the corresponding position on the belt\n",
    "        position_on_belt = (sum_direction_degrees % 360) * (belt_length / 360)\n",
    "\n",
    "        ###########\n",
    "        #plotting the histogram of the vector lengths coming from the shuffled data and compare it with the actual vector length\n",
    "        ###########\n",
    "\n",
    "        shuffled_vector_lengths=[]\n",
    "\n",
    "        for n in range(shuffling_times):\n",
    "            #shuffling every row separately\n",
    "            shuffled_events_data = events_data.copy()  # Make a copy to avoid modifying original data\n",
    "            for row in shuffled_events_data:\n",
    "                np.random.shuffle(row)  # Shuffle the spiking events within each round independently\n",
    "\n",
    "            # Calculate average direction and magnitude for shuffled data. same logic as before exactly\n",
    "            shuffled_x_coords = []\n",
    "            shuffled_y_coords = []\n",
    "            for i, events_round in enumerate(shuffled_events_data):\n",
    "                event_angles = angles[np.where(events_round == 1)]\n",
    "                event_radii = np.ones_like(event_angles) * (i + 1)  # Adjust the radius for each round\n",
    "                shuffled_x_coords.extend(np.cos(event_angles)/(i + 1))\n",
    "                shuffled_y_coords.extend(np.sin(event_angles)/(i + 1))\n",
    "\n",
    "            shuffled_sum_x = np.sum(shuffled_x_coords)/ peak_n\n",
    "            shuffled_sum_y = np.sum(shuffled_y_coords)/ peak_n\n",
    "            shuffled_sum_direction = np.arctan2(shuffled_sum_y, shuffled_sum_x)\n",
    "            shuffled_sum_magnitude = np.sqrt(shuffled_sum_x**2 + shuffled_sum_y**2)\n",
    "            shuffled_vector_lengths.append(shuffled_sum_magnitude)\n",
    "\n",
    "            # Map the shuffled average direction to the corresponding position on the belt\n",
    "            shuffled_position_on_belt = (np.degrees(shuffled_sum_direction) % 360) * (150 / 360)\n",
    "\n",
    "\n",
    "        ###########\n",
    "        #making the histogram with the shuffled data and our actual vector length\n",
    "        ###########\n",
    "\n",
    "        # Calculate histogram\n",
    "        counts, bins = np.histogram(shuffled_vector_lengths, bins=20)\n",
    "\n",
    "        # Calculate probabilities\n",
    "        total_samples = len(shuffled_vector_lengths)\n",
    "        probabilities = counts / total_samples\n",
    "\n",
    "        # Plot histogram with probabilities\n",
    "        ax[1,1].bar(bins[:-1], probabilities, width=np.diff(bins), color='blue', alpha=0.7)\n",
    "        ax[1,1].axvline(x=sum_magnitude,color='red')\n",
    "        ax[1,1].set_xlabel('Vector Length')\n",
    "        ax[1,1].set_ylabel('Probability')\n",
    "        # Remove right and upper axes\n",
    "        ax[1,1].spines['right'].set_visible(False)\n",
    "        ax[1,1].spines['top'].set_visible(False)\n",
    "        ax[1,1].set_title('Histogram for vector lengths')\n",
    "        ax[1,1].grid(False)\n",
    "\n",
    "\n",
    "        # comparing it with the actual vector lenth now)\n",
    "        # Actual vector length\n",
    "        actual_length = sum_magnitude  # Example value\n",
    "\n",
    "\n",
    "        # Calculate the p-value\n",
    "        larger_lengths_count = np.sum(np.array(shuffled_vector_lengths) > actual_length)\n",
    "        total_samples = len(shuffled_vector_lengths)\n",
    "        p_value_vector = larger_lengths_count / total_samples\n",
    "        # Annotate the p-value on the upper side of the plot\n",
    "\n",
    "        # Create a legend entry with the p-value\n",
    "        p_value_text = f'P-value: {p_value_vector:.4f}'\n",
    "        p_value_patch = mpatches.Patch(color='none', label=p_value_text)\n",
    "\n",
    "        # Add the legend with the custom entry\n",
    "        ax[1,1].legend(handles=[p_value_patch],loc='upper right', bbox_to_anchor=(1.25, 1.05),frameon=False)\n",
    "\n",
    "        ###################################################\n",
    "        #plotting the histogram from colmogorov smirnov test\n",
    "        ##################################################\n",
    "\n",
    "        data=cell_firing_rate_map\n",
    "        data_avg=np.mean(cell_firing_rate_map,axis=0)\n",
    "\n",
    "        ###########################\n",
    "        # I will shuffle once. and then I will compare the shufle data with my original data. the colmogorov smiron value will be the value that I will compare later\n",
    "        # later I will shuffle the data 1000 times and I will compare them to the first suffling data\n",
    "        ##############################\n",
    "\n",
    "        shuffled_ks=[] #array where I will put the ks distances where I will compare the shuffled data with my first shuffling\n",
    "        baseline=data.copy()\n",
    "\n",
    "        #shuffle 1 for the baseline\n",
    "        for i in range(num_rounds):\n",
    "            shuf=random.randint(1,150)\n",
    "            baseline[i]=np.roll(baseline[i],shuf)\n",
    "        baseline_avg=np.mean(baseline,axis=0)\n",
    "        \n",
    "        baseline_ks,_=kstest(data_avg,baseline_avg)\n",
    "\n",
    "\n",
    "        # now I will shuffle many times and then compare\n",
    "          \n",
    "        for n in range(1,shuffling_times):\n",
    "            data_shuffle=data.copy()    \n",
    "            for i in range(num_rounds):\n",
    "                shuf=random.randint(1,150)\n",
    "                data_shuffle[i]=np.roll(data_shuffle[i],shuf)\n",
    "            \n",
    "            \n",
    "\n",
    "            data_shuffle=np.mean(data_shuffle,axis=0)\n",
    "            ks_shuffle,p_value_=kstest(baseline_avg,data_shuffle)\n",
    "            shuffled_ks.append(ks_shuffle)\n",
    "\n",
    "        \n",
    "        # Calculate histogram\n",
    "        counts, bins = np.histogram(shuffled_ks, bins=30)\n",
    "\n",
    "        # Calculate probabilities\n",
    "        total_samples = len(shuffled_ks)\n",
    "        probabilities = counts / total_samples\n",
    "        # Calculate the p-value\n",
    "        larger_lengths_count = np.sum(np.array(shuffled_ks) > baseline_ks)\n",
    "        p_value_ks = larger_lengths_count / total_samples\n",
    "        # Annotate the p-value on the upper side of the plot\n",
    "        # Create a legend entry with the p-value\n",
    "        p_value_text = f'P-value: {p_value_ks:.4f}'\n",
    "        p_value_patch = mpatches.Patch(color='none', label=p_value_text)\n",
    "\n",
    "        # Add the legend with the custom entry\n",
    "        ax[2,1].legend(handles=[p_value_patch],loc='upper right', bbox_to_anchor=(1.25, 1.05),frameon=False)\n",
    "\n",
    "        ax[2,1].bar(bins[:-1], probabilities, width=np.diff(bins), color='blue', alpha=0.7)\n",
    "        ax[2,1].axvline(x=baseline_ks,color='red')\n",
    "        ax[2,1].set_xlabel('ks Length')\n",
    "        ax[2,1].set_ylabel('Probability')\n",
    "        # Remove right and upper axes\n",
    "        ax[2,1].spines['right'].set_visible(False)\n",
    "        ax[2,1].spines['top'].set_visible(False)\n",
    "        ax[2,1].set_title('colmogorov smirnov test')\n",
    "        ax[2,1].grid(False)\n",
    "\n",
    "        \n",
    "            # Check the p-value and save the figure to the appropriate PDF file\n",
    "        if p_value_vector < 0.05 and p_value_ks < 0.05:\n",
    "            place_cells.append(cell)\n",
    "            significant_cells.savefig(fig)\n",
    "        else:\n",
    "            non_significant_cells.savefig(fig)\n",
    "\n",
    "    plt.ioff() #this is so I dont have to show the plots but only store them\n",
    "\n",
    "# Close the PDF files after saving all figures\n",
    "significant_cells.close()\n",
    "non_significant_cells.close()\n",
    "\n",
    "place_cell_per_cent=100* len(place_cells)/units_n\n",
    "print(f\"place cell percentage {place_cell_per_cent} %\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating the plae code diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "place cell percentage 9.780775716694773 %\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
